---
title: "Decision Trees"
author: "Megan Lin, James Powell, Eva Mustafix"
date: "May 3rd, 2021"
output: 
  html_document:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE, message = FALSE)
```

Congrats! You just graduated from medical school and got a PhD in Data Science at the same time, wow impressive. Because of these incredible accomplishments the world now believes you will be able to cure cancer...no pressure. To start you figured you better create some way to detect cancer when present. Luckily because you are now a MD and DS PhD or MDSDPhD, you have access to data sets and know your way around a ML classifier. So, on the way to fulfilling your destiny to rig the world of cancer you start by building several classifiers that can be used to aid in determining if patients have cancer and the type of tumor. 

The included dataset (clinical_data_breast_cancer_modified.csv) has information 
on 105 patients across 17 variables, your goal is to build two classifiers one 
for PR.Status (progesterone receptor), a biomarker that routinely leads to a 
cancer diagnosis, indicating if there was a positive or negative outcome and 
one for the Tumor a multi-class variable . You would like to be able to explain 
the model to the mere mortals around you but need a fairly robust and flexible 
approach so you've chosen to use decision trees to get started. In building both
models us CART and C5.0 and compare the differences. 

In doing so, similar to great data scientists of the past, you remembered 
the excellent education provided to you at UVA in a 
undergrad data science course and have outlined steps that will need to be 
undertaken to complete this task (you can add more or combine if needed).  
As always, you will need to make sure to #comment your work heavily and 
render the results in a clear report (knitted) as the non MDSDPhDs of the 
world will someday need to understand the wonder and spectacle that will 
be your R code. Good luck and the world thanks you. 

 Footnotes: 
-	Some of the steps will not need to be repeated for the second model, use your judgment
-	You can add or combine steps if needed
-	Also, remember to try several methods during evaluation and always be mindful 
of how the model will be used in practice.
- Do not include ER.Status in your first tree it's basically the same as PR.Status

Libraries
```{r echo = FALSE, include = FALSE}
library(rio)
library(plyr)
library(dplyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
#setwd("/cloud/project/decision_trees")
library(caret)
#install.packages("C50")
#install.packages("mlbench")
#install.packages("e1071")
library(e1071)
library(C50)
library(mlbench)
```

## Reading in the Data
```{r}
#1 Load the data and ensure the column names don't have spaces, hint check.names.  
data <- tibble(import("clinical_breast_cleaned.csv", check.names= TRUE))
```
 First, we load in the data and drop the ER.Status column as specified in the instructions. We also ensure all the variables are classified correctly and ensure that the target variable, PR.Status, is changed to a numeric 0 (negative) and 1 (positive) system. 
 
```{r}
#2 Ensure all the variables are classified correctly and ensure the target variable for "PR.Status" #is 0 for negative and 1 for positive

data$PR.Status <- as.numeric(data$PR.Status)
data$Age.at.Initial.Pathologic.Diagnosis <- as.numeric(data$Age.at.Initial.Pathologic.Diagnosis)
data$Days.to.Date.of.Last.Contact <- as.numeric(data$Days.to.Date.of.Last.Contact)
data$Days.to.date.of.Death <- as.numeric(data$Days.to.date.of.Death)
data$OS.Time <- as.numeric(data$OS.Time)
```

```{r}
#3 Don't check for correlated variables....because it doesn't matter with Decision Trees...that was easy
```

## Splitting
Next, we split the data into a training and a testing set
```{r}
#4 Split your data into test and train using the caret
x <- createDataPartition(data$PR.Status,times=1,p = 0.8,list=FALSE)
training <- data[x,-3]
test <- data[-x,-3]
```

Training Data
```{r}
head(training)
```

Testing Data
```{r}
head(test)
```

```{r}

#5 Guess what, you also don't need to standardize the data, because DTs don't 
# give a ish, they make local decisions...keeps getting easier 

```

## Baserate
```{r}
#6 Ok now determine the baserate for the classifier, what does this number mean.  
#For the multi-class this will be the individual percentages for each class. 
data_long = data %>% gather(Var, #<- list of predictor variables
                                Value,#<- the values of those predictor variables
                                -PR.Status) 
data_long_form = ddply(data_long, 
                            .(Var, Value),#<- group by Var and Value, "." 
                            #allows us to call the variables without quoting
                            summarize,  
                            prob_PR.Status = mean(PR.Status), #<- probability of being Parent
                            prob_not_PR.Status = 1 - mean(PR.Status)) #<- probability of not being Parent
avg= mean(data[["PR.Status"]])
1-avg
```
The baserate is the base class of probabilities unconditioned on featural evidence, frequently also known as prior probabilities. Because this is a multiclass, we will be using the individual percentages for each class.

We first gather/pair the data by the predictor variables (e.g. gender, tumor metastasis, etc.), then by the value of those predictor variables. We also add the probability that their PR Status is positive vs. negative.

The overall baserate was found by taking the average of values in the PR.Status column. Thus, we found that 51.4% of the testing data had a positive PR Status and 48.6% had a negative PR Status.

## Building the Model

We build our model using the default setting and by using the rpart function. PR.Status is used as the "formula" aka our response variable. We utilize our previouslt split training dataset and set a cp of 0.01 as it is our default.

```{r}
#7 Build your model using the default settings
set.seed(2702)
data_gini = rpart(PR.Status~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training,#<- data used
                            control = rpart.control(cp=.01))
```

```{r}
#8 View the results, what is the most important variable for the tree?
#Look at the results
data_gini
```

Now we can see the probabilities of being positive or negative based on certain "decisions" or factors. 

The most important variable is shown to be the first split in our decision tree the ACJCC Stage (only if they are stages IIA, III, IV). In descending order, the next three most important factors are the Her2 Final Status, Converted Stage, and OS Time.

This method of visualization isn't the most easy to use though so let's move on to making it a more visually pleasing tree!

## Decision Tree

```{r}
#9 Plot the tree using the rpart.plot package (CART only).
rpart.plot(data_gini, type =4, extra = 101)
```

Much better! Here we can see the split of importance of the variables and their end predictions. For example, if we knew the patient had a converted stage of Stage I breast cancer, we would follow the tree to the rightmost branch and immediately land at a terminal node.

## CP Chart

```{r}
#10 plot the cp chart and note the optimal size of the tree (CART only).
plotcp(data_gini)

cptable_ex <- as_tibble(data_gini$cptable)
cptable_ex
```
Next, we produce a "elbow chart" for various cp values and their associated relative errors. The dashed line represents the highest cross-validated error minus the minimum cross-validated error, plus the standard deviation of the error at that tree. A reasonable choice of cp for pruning is  the leftmost value because this is where the mean is less than the horizontal line. 

The optimal number of splits is 3 as shown in the cptable_ex because this is where the chart first dips below the mean line. Any more splitting increases the x-val relative error. Furthermore, we can also see in the cptable that this has a low relative error, okay xerror, and a decent xstd

### Variable Importance

```{r}
data_gini$variable.importance
```
We can also visualize the importance of including each variable as shown above. This reiterates similar points to what we have previously found, but displays the relative importance for all of the variables now. Most important is the Converted Stage with a top score of 3.24 followed by AJCC stage with a score of 3.21. At the bottom and being the least important are node coded with a score of 0.22 and metastasis than 0.185.

Next, we use the predict function to predict the target variable.
```{r}
#11 Use the predict function and your models to predict the target variable using
#test set. 
tree_predict = predict(data_gini,test, type= "class")

tree_predict
```
Our test set of size 21 resulted in the prediction of the target variable, the PR Status, as shown above.

## Confusion Matrix
### Hit and Detection Rate
```{r}
#12 Generate, "by-hand", the hit rate and detection rate and compare the 
#detection rate to your original baseline rate. How did your models work?
confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

```
The detection rate is found to be 38.10% with a prevalence of 57.1% Our model currently has an accuracy of 66.67%.

The hit rate can be derived from the confusion matrix by taking the number of true positives (8) over total actual positive cases (8+ a missing 4 = 12) for a hit rate of 66.7%.

### Best Metrics
```{r}
#13 Use the the confusion matrix function in caret to 
#check a variety of metrics and comment on the metric that might be best for 
#each type of analysis.  
par_conf_matrix<-confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```

```{r}
par_error_rate = (par_conf_matrix$table[1,2]+par_conf_matrix$table[2,1]) / (par_conf_matrix$table[1,2]+par_conf_matrix$table[2,1]+par_conf_matrix$table[1,1]+par_conf_matrix$table[2,2])

par_error_rate
```
The Hit Rate/ True Error Rate is 33.3%

```{r}
true_pos_rate = par_conf_matrix$table[2,2]/(par_conf_matrix$table[1,2]+par_conf_matrix$table[2,1]+par_conf_matrix$table[1,1]+par_conf_matrix$table[2,2])
true_pos_rate 
```
The true positive rate, also shown as our detection rate is 38.10%

The true positive rate could be improved upon as the ideal would create a confusion matrix with a value of 0 in the spot (1,2) also meaning a hit rate of 100%. We would want to focus on a higher hit rate in this specific case because it would be better for the patients to be more careful about their breast cancer diagnosis with a false positive rather than it being kept hidden with a false negative reading. If keeping false positives low was the ideal metric, then we would ideally be looking for a value of 0 in the confusion matrix at the spot (0,0) and also aiming for a high specificity or true negative rate.

## AUC - ROC Curve
```{r}
#14 Generate a ROC and AUC output, interpret the results
par_roc <- roc(test$PR.Status, as.numeric(tree_predict), plot = TRUE)
par_roc
plot(par_roc)
```

AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s.

The area under the curve of this graph is 0.6667. As our specificity decreases from 1.0 to 0.5, our sensitivity increases at a high rate through a specificity of 0.5, then the incremental gain drop for a specificity of 0.5 to 0. 

As shown in the graph, there is a slight inflection point caused by the two rates that causes a deviation from the baseline, but a larger deviation from the line is more desirable and indicates better predicting qualities by the model.

## Multi Class Target: Tumors

```{r}
#15 Follow the same steps for the multi-class target, tumor, aside from step 1, 
# 2 and 14. For step 13 compare to the four base rates and see how you did. 

data2 <- read.csv("clinical_breast_cleaned.csv")
data2$Tumor <- as.factor(data2$Tumor)
data2$Age.at.Initial.Pathologic.Diagnosis <- as.numeric(data2$Age.at.Initial.Pathologic.Diagnosis)
data2$Days.to.Date.of.Last.Contact <- as.numeric(data2$Days.to.Date.of.Last.Contact)
data2$Days.to.date.of.Death <- as.numeric(data2$Days.to.date.of.Death)
data2$OS.Time <- as.numeric(data2$OS.Time)

data2$Tumor1 <- as.numeric(ifelse(data2$Tumor == "T1", 1,0))
data2$Tumor2 <- as.numeric(ifelse(data2$Tumor == "T2", 1,0))
data2$Tumor3 <- as.numeric(ifelse(data2$Tumor == "T3", 1,0))
data2$Tumor4 <- as.numeric(ifelse(data2$Tumor == "T4", 1,0))

```

### Splitting
Next, we split the data into a training and a testing set
```{r}
#4 Split your data into test and train using the caret
x2 <- createDataPartition(data2$Tumor,times=1,p = 0.8,list=FALSE)
training2 <- data[x,-3]
test2 <- data[-x,-3]
```

### Baserate

```{r}
#base rate

tumor_long = data2 %>% gather(Var, #<- list of predictor variables
                                Value,#<- the values of those predictor variables
                                -Tumor) 
tumor_long_form = ddply(tumor_long, 
                            .(Var, Value),#<- group by Var and Value, "." 
                            #allows us to call the variables without quoting
                            summarize,  
                            prob_T1 = mean(data2$Tumor1),
                            prob_T2 = mean(data2$Tumor2),
                            prob_T3 = mean(data2$Tumor3),
                            prob_T4 = mean(data2$Tumor4))

tumor1_avg= mean(data2[["Tumor1"]])
tumor2_avg= mean(data2[["Tumor2"]])
tumor3_avg= mean(data2[["Tumor3"]])
tumor4_avg= mean(data2[["Tumor4"]])
```
The overall baserates for each type of tumor was found by taking the average of values in the tumor# column. 

Tumor 1 has a 14.3% positive rate, 2 has a 61.9% positive rate, 3 has a 18.1% positive rate, and 4 has a 5.7% positive rate.

### Building the Model

We build our model using the default setting and by using the rpart function. PR.Status is used as the "formula" aka our response variable. We utilize our previously split training dataset and set a cp of 0.01 as it is our default.

```{r}
#7 Build your model using the default settings
set.seed(2702)
data2_gini = rpart(Tumor~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training,#<- data used
                            control = rpart.control(cp=.01))

```

```{r}
#8 View the results, what is the most important variable for the tree?
#Look at the results
data2_gini
```
Now we can see the probabilities of being positive or negative based on certain "decisions" or factors. 

The most important variable is shown to be the first split in our decision tree, the AJCC Stage (specifically stages I, IA, and IIIB). In order of descending importance are the second group of AJCC Stages (IB, II, IIA, IIB, III), the node coding positive or negative, and the days to the date of last contact..

This method of visualization isn't the most easy to use though so let's move on to making it a more visually pleasing tree!

## Decision Tree

```{r}
#9 Plot the tree using the rpart.plot package (CART only).
rpart.plot(data2_gini, type =4, extra = 101)
```

Much better! Here we can see the split of importance of the variables and their end predictions. For example, if we knew the patient were in AJCC Stage I, we would say they likely have a T1 tumor. Interestingly here, the T4 tumor is unused because it wasn't found in enough of the data (it does not have the predominant number in any grouping -- it challenges the reading of T1 in the rightmost node with 5, but T1 had slightly more with 7 readings).

## CP Chart

```{r}
#10 plot the cp chart and note the optimal size of the tree (CART only).
plotcp(data2_gini)

cptable_ex <- as_tibble(data2_gini$cptable)
cptable_ex
```
Next, we produce a "elbow chart" for various cp values and their associated relative errors. The dashed line represents the highest cross-validated error minus the minimum cross-validated error, plus the standard deviation of the error at that tree. A reasonable choice of cp for pruning is  the leftmost value because this is where the mean is less than the horizontal line. 

The optimal number of splits is 2 as shown in the cptable_ex because this is where the chart first dips below the mean line. Any more splitting increases the x-val relative error. Furthermore, we can also see in the cptable that this has a low relative error, low xerror, and a good xstd

### Variable Importance

```{r}
data2_gini$variable.importance
```
We can also visualize the importance of including each variable as shown above. This reiterates what we have previously found, but displays the relative importance for all of the variables now. Most important is the AJCC Stage with a top score of 17.3 followed by converted stage with a score of 12.5. At the bottom and being the least important are Her2 final status with a score of 0.847 and survival data form with a score of 0.847.

Next, we use the predict function to predict the target variable.
```{r}
#11 Use the predict function and your models to predict the target variable using
#test set. 
tree_predict2 = predict(data2_gini,test, type= "class")

tree_predict2
```
Our test set of size 21 resulted in the prediction of the target variable, the type of tumor, as shown above.

## Confusion Matrix
### Hit and Detection Rate
```{r}
#12 Generate, "by-hand", the hit rate and detection rate and compare the 
#detection rate to your original baseline rate. How did your models work?
confusionMatrix(as.factor(tree_predict2), as.factor(test2$Tumor), 
                dnn=c("Prediction", "Actual"), mode = "sens_spec")

#table(test2$Tumor)
```
The detection rates range from 0% with T4 to 38% with T2. 

The hit rate can be derived from the confusion matrix by taking the number of true positives for each type of tumor over total actual positive cases.

Tumor 1: Correct Hits = 1, Actual = 3 --> Hit Rate = 33.3%
Tumor 2: Correct Hits = 8, Actual = 10 --> Hit Rate = 80%
Tumor 3: Correct Hits = 3, Actual = 5 --> Hit Rate = 60%
Tumor 4: Correct Hits = 0, Actual = 3 --> Hit Rate = 0%

We have a good hit rate for tumor 2, an okay rate for tumor 3, a bad rate for tumor 1, and a terrible rate for tumor 4. More research money should be invested in improving detection for tumor 4.

### Best Metrics
```{r}
#13 Use the the confusion matrix function in caret to 
#check a variety of metrics and comment on the metric that might be best for 
#each type of analysis.  
par_conf_matrix2<-confusionMatrix(as.factor(tree_predict2), as.factor(test2$Tumor), 
                dnn=c("Prediction", "Actual"), mode = "sens_spec")


```

```{r}
#par_error_rate2 = sum(par_conf_matrix2[row(par_conf_matrix2) != col(par_conf_matrix2)]) / sum(par_conf_matrix2)

#par_error_rate2
```
This rate can be found by summing the values in the confusion matrix where the rows do not equal the columns indicating errors on the model's part and dividing it by the sum of all instances in the confusion matrix.

This gives us a Hit Rate/ True Error Rate of 9/21 which is is 42.9%

```{r}
#true_pos_rate = par_conf_matrix$table[2,2]/(par_conf_matrix$table[1,2]+par_conf_matrix$table[2,1]+par_conf_matrix$table[1,1]+par_conf_matrix$table[2,2])
#true_pos_rate 
```
The true positive rate, also shown as our detection rate can be found by summing the values along the diagonal of the confusion matrix and dividing that value by all values in the confusion matrix which gives us 12/21 which is 57.1%

## Lessons Learned
```{r}
# 16 Summarize what you learned for each model along the way and make 
# recommendations to the world on how this could be used moving forward, 
# being careful not to over promise. 
```
Within each model we were able to create models that could read a patient's information and give an evaluation on the patient's PR status in the first model and what type of tumor they have in the second.

Within the first model, we learned that the most important factors to look for to determine this is the patient's AJCC Stage, Her2 Final Status, Converted Stage, and OS Time. 

Similarly within the second model to determine Tumor type, the most important factors are the AJCC Stage (specifically stages I, IA, and IIIB), the second group of AJCC Stages (IB, II, IIA, IIB, III), the node coding positive or negative, and the days to the date of last contact.

We can see an overlap of AJCC stage ranking highly between both so this could be of something to note for doctors to pay more attention to during diagnosis.

We would recommend data scientists to iterate over this model to work to decrease the false negative rate for these models. Because of the nature of its usage, it is more detrimental for a patient to get a false negative and go undiagnosed than a patient to get a false positive and to begin treatment for something they do not have.

Because of the less than ideal hit rate/true error rate and true positive rates, we would not recommend these models in professional use, but do see the value of gaining more data to add to this dataset to continuously improve the model.

The size of this dataset was rather small which lead to many issues such as within the tumor model, tumor 4 was unrepresented throughout and was not used in our decision tree. We would want more data on patients with tumor 4 to better represent them within our dataset to get it to even show up as an option within our decision tree.